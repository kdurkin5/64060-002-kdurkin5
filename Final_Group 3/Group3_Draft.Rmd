---
title: "Final_Group3"
author: "Alexis McCartney, Marvelle Horton, Kristen Durkin"
date: "`r Sys.Date()`"
output: word_document
---

# Process
Load Data -> Clean Data -> Scale Data -> Split Data -> Pick K -> Run K -> Analysis -> Visuals

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Data

This file contains the YTD variables we will use for clustering for 2023.

```{r}
df <- read.csv("Group3Data.csv") #read csv file 
View(df) #Visual Check
```

# Step 1: Remove variables that have significant missing values

Check and see if there are any values missing in each column:

```{r}
colSums(is.na(df)) #Count N/A values in each column
```
Returned data does not show significant missing values for the YTD variables.

# Step 2: Focus on last 6 YTD column 
(First we count all the columns then select the last 6)

```{r}
num_cols <- ncol(df)   #total columns
ytd <- df[, (num_cols-5):num_cols] #grab last 6 columns (YTD data)
head(ytd)
colSums(is.na(ytd)) #double check for missing values
```

Results show that the YTD columns do not have any missing values.

YTD Columns:
X.90 = Total Fuel Consumption Quantity
X.91 = Electric Fuel Consumption Quantity
X.92 = Total Fuel Consumption MMBtu
X.93 = Elec Fuel Consumption MMBtu
X.94 = Net Generation (Megawatthours)
X.95 = YEAR

# Step 3: Remove YEAR from clustering data since it's constant

```{r}
ytd_noyear <- ytd[, c("X.90", "X.91", "X.92", "X.93", "X.94")] #Drop YEAR column since it's constant
head(ytd_noyear) #confirm remaining columns
```

# Step 4: Remove any non-numerical data from the set

```{r}
ytd_clean <- ytd_noyear[!is.na(as.numeric(ytd_noyear$X.90)), ] #keep only numerical rows
head(ytd_clean) #check the cleaned data
```

# Step 5: Remove any special characters like commas

```{r}
ytd_clean <- as.data.frame(lapply(ytd_clean, function(x) as.numeric(gsub(",", "", x)))) #remove special characters
str(ytd_clean)
```

Now the data is all numerical & clean.

# Step 6: Scale the data
We scale so that all features contribute equally to the k-means distance calculation.

```{r}
ytd_scaled <- scale(ytd_clean)
head(ytd_scaled)
str(ytd_scaled) #confirm data is all numbers
```

Scaling makes our mean roughly 0 and standard deviation roughly 1 (z-scores)

```{r}
colMeans(ytd_scaled) #Confirmation of scale
```

```{r}
apply(ytd_scaled, 2, sd) #Confirmation of standard deviation
```

# Step 7: Split into training/test data sets

Split the scaled dataset into a training set (75% of observations) and a test set (25%). The training set will be used to determine the optimal number of clusters and to build the k-means segmentation.

```{r}
set.seed(123) 
n <- nrow(ytd_scaled)
train_idx <- sample(1:n, size = 0.75 * n) #randomly select 75% of data for training

train <- ytd_scaled[train_idx, ] #training set
test <- ytd_scaled[-train_idx, ] #test set

dim(train) #check training dimensions
dim(test) #check test dimensions
```

Confirmation: data is split.

# Step 8: Elbow Plot

The elbow method helps determine the number of clusters by plotting the total within-cluster sum of squares (WSS) for different values of k.

```{r}
wss <- sapply(1:10, function(k) {
  kmeans(train, centers = k, nstart = 20)$tot.withinss #get total WSS for each k 
})

plot(1:10, wss, type = "b",
     xlab = "Number of Clusters (k)",
     ylab = "Within-Cluster Sum of Squares (WSS)",
     main = "Elbow Method") 
#look where elbow - this shows when adding more would not be benefical 
``` 

Results:

Cluster 1 = very high WSS with everything lumped together.
k = 2 = largest change.
k = 3 = “elbow” point.

Using the elbow method,a clear bend at k = 3 can be seen. While the WSS decreases sharply from k = 1 to k = 3, the reduction beyond k = 3 is minimal. This indicates that 3 clusters provide an ideal balance.


# Step 9: Silhouette (Confirmation)

```{r}
library(cluster)

avg_sil <- sapply(2:10, function(k){
  km <- kmeans(train, centers = k, nstart = 20) #Run k means 
  ss <- silhouette(km$cluster, dist(train)) #run silhouette for clustering
  mean(ss[, 3]) #avg silhouette for k
  })

plot(2:10, avg_sil, type = "b",
    xlab = "Number of Clusters (k)",
    ylab = "Average Silhouette width",
    main = "Silhouette Method")

#Higher values mean more defined clusters
```

Confirmation k = 3

Based on the elbow method, it's observed to have a clear bend at k = 3, indicating diminishing returns beyond this point. Although the silhouette method peaked at k = 2, the value for k = 3 remained high, suggesting well separated clusters.

For this reason we selected k = 3 as the optimal number of clusters.

# Step 10: Run K-means with k = 3
Now we actually run the final clustering model using k = 3.

```{r}
set.seed(123)
km_final <- kmeans (train, centers = 3, nstart = 25) #final k means model

km_final$size #how many plants are in each cluster
km_final$centers #cluster center scaled
```

# Step 11: Cluster Interpretation

Cluster sizes:
Cluster 1: 8845 plants
Cluster 2: 308 plants
Cluster 3: 72 plants

Cluster 1 -> Low values

Slightly lower than average fuel use
Slightly lower than average heat content
Slightly lower electricity generation

Cluster 1 contains units with below average fuel consumption and below average electricity generation.
This suggests these units operate at relatively low levels of activity compared to the rest of the dataset.

Cluster 2 -> Mid-high values

Higher than average fuel use
Mid range fuel energy content
Mid range generation levels

Cluster 2 shows higher fuel consumption than Cluster 1 but does not reach the higher levels seen in Cluster 3.
These units demonstrate moderate fuel use and moderate electricity output

Cluster 3 ->  high values

Extremely high fuel consumption
Extremely high MMBtu
Extremely high electricity generation

Cluster 3 onsists of the highest intensity units in the dataset. They consume substantially more fuel and generate much more electricity than the other clusters. Although small in count, these units represent the most operational activity.

# Step 12: Add IDs and Visualize

Now we attach the cluster labels back to the training data and plot the clusters.

```{r}
train_clusters <- as.data.frame(train) #transition from training to data frame
train_clusters$cluster <- km_final$cluster #add labels to clusters
head(train_clusters) #data preview
```

```{r}
library(factoextra) #package for easy k-means cluster visualizations

fviz_cluster(
  list(data = train, cluster = km_final$cluster),
  geom = "point", #show each plant as a point
  ellipse.type = "convex", # draw convex hulls around clusters (Mod 6)
  main = "K-Means Clustering (k = 3)" 
)
# This visual reduces our 5 scaled variables into a 2D space so we can actually see how well the clusters separate.
# The convex ellipses help outline the grouping structure,which is exactly how we learned to interpret cluster quality in mod 6.
```

#
